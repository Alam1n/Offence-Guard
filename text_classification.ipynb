{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muham\\AppData\\Local\\Temp\\ipykernel_13988\\3238513867.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  combined_data['oh_label'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muham\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8411/8411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 21ms/step - accuracy: 0.9088 - loss: 0.2444 - val_accuracy: 0.9310 - val_loss: 0.1946\n",
      "Epoch 2/3\n",
      "\u001b[1m8411/8411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 17ms/step - accuracy: 0.9347 - loss: 0.1707 - val_accuracy: 0.9391 - val_loss: 0.1588\n",
      "Epoch 3/3\n",
      "\u001b[1m8411/8411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 15ms/step - accuracy: 0.9408 - loss: 0.1553 - val_accuracy: 0.9398 - val_loss: 0.1599\n",
      "\u001b[1m2629/2629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9423 - loss: 0.1551\n",
      "Test Loss: 0.15797485411167145\n",
      "Test Accuracy: 0.9412170648574829\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "Offensive\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load datasets\n",
    "twitter_data = pd.read_csv('Hate_speech_folder/twitter_parsed_dataset.csv')\n",
    "aggressive_data = pd.read_csv('Hate_speech_folder/aggression_parsed_dataset.csv')\n",
    "kaggle_data = pd.read_csv('Hate_speech_folder/kaggle_parsed_dataset.csv')\n",
    "youtube_parsed =pd.read_csv('Hate_speech_folder/youtube_parsed_dataset.csv')\n",
    "attack_data = pd.read_csv(\"Hate_speech_folder/attack_parsed_dataset.csv\")\n",
    "toxic_data = pd.read_csv(\"Hate_speech_folder/toxicity_parsed_dataset.csv\")\n",
    "\n",
    "# Combine datasets (assuming they all have 'Text' and 'oh_label' columns)\n",
    "combined_data = pd.concat([twitter_data[['Text', 'oh_label']],\n",
    "                            aggressive_data[['Text', 'oh_label']],\n",
    "                            youtube_parsed[['Text', 'oh_label']],\n",
    "                            toxic_data[['Text', 'oh_label']],\n",
    "                            attack_data[['Text', 'oh_label']],\n",
    "                            kaggle_data[['Text', 'oh_label']]])\n",
    "\n",
    "# Handle NaN values in 'oh_label'\n",
    "combined_data['oh_label'].fillna(0, inplace=True)\n",
    "combined_data['oh_label'] = combined_data['oh_label'].astype(int)\n",
    "\n",
    "# Tokenize and pad text data\n",
    "texts = combined_data['Text'].astype(str).values\n",
    "labels = combined_data['oh_label'].values\n",
    "\n",
    "max_words = 10000  # Adjust as needed\n",
    "max_len = 100      # Adjust as needed\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Split the combined data\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')  # Binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=3, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# Function to predict if a text is offensive\n",
    "def predict_offensive(text):\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    return 'Offensive' if prediction[0][1] > 0.5 else 'Not Offensive'\n",
    "\n",
    "# Example prediction\n",
    "print(predict_offensive('Fuck you'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Cyber_bullying_detection_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Tokenize and pad text data\n",
    "texts = combined_data['Text'].astype(str).values\n",
    "labels = combined_data['oh_label'].values\n",
    "\n",
    "max_words = 10000  # Adjust as needed\n",
    "max_len = 100      # Adjust as needed\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Save the tokenizer to a file\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
